{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do TF-IDF for these three months in the two datasets\n",
    "# The tokenizer packages\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopWords = set(stopwords.words('english')+ list(string.punctuation))\n",
    "\n",
    "# The TF-IDF packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Doing TF-IDF on the body of that month. \n",
    "def robustParse(text,replaceNL=True):\n",
    "    \"\"\"Helper function for BeautifulSoup parsing of comment text\"\"\"\n",
    "    \n",
    "    try: \n",
    "        if replaceNL: NL = \"\\n\"\n",
    "        else: NL = \" \"\n",
    "        return bs4.BeautifulSoup(text, \"lxml\").text.replace(f\"{NL}\",\" \")\n",
    "    except: \n",
    "        return None \n",
    "    \n",
    "def tokenize(text,remove_stop=True, lower=True, stem=True, sep=\" \"):\n",
    "    \"\"\"Function to consolidate a number of text cleaning methods\"\"\"\n",
    "    \n",
    "    if lower: text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)  #  Use NLTK's tokenizer\n",
    "    \n",
    "    if remove_stop:\n",
    "        tokens = [item for item in tokens if item not in stopWords]\n",
    "\n",
    "    if stem:\n",
    "        return sep.join([PorterStemmer().stem(item) for item in tokens])\n",
    "\n",
    "    else:\n",
    "        return sep.join(tokens)\n",
    "\n",
    "    \n",
    "def getTFIDF(text_series):\n",
    "    \"\"\"Performs TF-IDF calculation on a matrix of words grouped into cells.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r'[^\\s]+')\n",
    "\n",
    "    matrix = vectorizer.fit_transform(text_series).todense()\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, columns=vectorizer.get_feature_names())\n",
    "\n",
    "    # sum over each document (axis=0)\n",
    "    return matrix_df.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gme12529 = pd.read_csv('gme/gme12529.csv',dtype=str)\n",
    "cgme12529 = gme12529[(gme12529['Body']!='[removed]') & (gme12529['Body']!='[deleted]') & (gme12529['Author']!='[deleted]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cgme12529)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gme12529)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgme12529['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1gme12529 = cgme12529[~cgme12529['Body'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cgme12529[\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1corpus = pd.concat([cgme12529[\"Title\"],c1gme12529['Body']])\n",
    "peak1corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1topwords = getTFIDF(peak1corpus.map(tokenize))\n",
    "peak1topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1topwordsdf = pd.DataFrame(peak1topwords)\n",
    "peak1topwordsdf.to_csv('/Users/elaine/Desktop/peak1topwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,5001,1000):\n",
    "    exec(f\"peak1por{i} = pd.read_csv('1st gme comments/random/gme12529author{i}.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,27001,1000):\n",
    "    exec(f\"peak1po{i} = pd.read_csv('1st gme comments/gme12529authorrest{i}.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1po27814 = pd.read_csv('1st gme comments/gme12529authorrest27814.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1aucomments = pd.concat([peak1por1000, peak1por2000,peak1por3000,peak1por4000,peak1por5000,peak1po1000,peak1po2000\n",
    "                          ,peak1po3000,peak1po4000,peak1po5000,peak1po6000,peak1po7000,peak1po8000,peak1po9000,peak1po10000\n",
    "                         ,peak1po11000,peak1po12000,peak1po13000,peak1po14000,peak1po15000,peak1po16000,peak1po17000\n",
    "                         ,peak1po18000,peak1po19000,peak1po20000,peak1po21000,peak1po22000,peak1po23000,peak1po24000\n",
    "                          ,peak1po25000,peak1po26000,peak1po27000,peak1po27814])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1aucomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1aucomments.Author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time commented in wsb\n",
    "from datetime import datetime\n",
    "peak1aucomments[\"Publish Date\"] = peak1aucomments['Publish Date'].map(lambda datetext: datetime.strptime(datetext,'%Y-%m-%d %H:%M:%S') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1au1stcom = peak1aucomments.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "peak1aulastcom = peak1aucomments.sort_values(\"Publish Date\",ascending=False).drop_duplicates(\"Author\",keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumpeak1au = peak1au1stcom[['Author','Publish Date']]\n",
    "sumpeak1au.rename(columns={'Author':'Author','Publish Date':'first_commented_in_wsb'},inplace = True)\n",
    "len(sumpeak1au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au = sumpeak1au.merge(peak1aulastcom[['Author','Publish Date']],left_on='Author',right_on='Author',how='left')\n",
    "sumpeak1au.rename(columns={'Publish Date':'last_commented_in_wsb'},inplace=True)\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,5001,1000):\n",
    "    exec(f\"peak1por{i} = pd.read_csv('1st gme posts/random/gme12529authorpost{i}.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000,27001,1000):\n",
    "    exec(f\"peak1po{i} = pd.read_csv('1st gme posts/gme12529restaupost{i}.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1po27814 = pd.read_csv('1st gme posts/gme12529restaupost27814.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1auposts = pd.concat([peak1por1000, peak1por2000,peak1por3000,peak1por4000,peak1por5000,peak1po1000,peak1po2000\n",
    "                          ,peak1po3000,peak1po4000,peak1po5000,peak1po6000,peak1po7000,peak1po8000,peak1po9000,peak1po10000\n",
    "                         ,peak1po11000,peak1po12000,peak1po13000,peak1po14000,peak1po15000,peak1po16000,peak1po17000\n",
    "                         ,peak1po18000,peak1po19000,peak1po20000,peak1po21000,peak1po22000,peak1po23000,peak1po24000\n",
    "                          ,peak1po25000,peak1po26000,peak1po27000,peak1po27814])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1auposts[\"created\"] = peak1auposts['created'].map(lambda datetext: datetime.strptime(datetext,'%Y-%m-%d %H:%M:%S') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1auposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1auposts.author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first time posted in wsb\n",
    "peak1au1stpo = peak1auposts.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1au1stpo = peak1au1stpo[['author','created']]\n",
    "sumpeak1au = sumpeak1au.merge(peak1au1stpo[peak1au1stpo.created < '2021-01-30 00:00:00'],left_on='Author',right_on='author',how='outer')\n",
    "sumpeak1au.rename(columns={'created':'first_posted_in_wsb'},inplace = True)\n",
    "sumpeak1au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au = sumpeak1au[~sumpeak1au.author.isna()]\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sumpeak1au['Author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#last time posted in wsb\n",
    "peak1aulastpo = peak1auposts.sort_values(\"created\",ascending=False).drop_duplicates(\"author\",keep='first')\n",
    "sumpeak1au = sumpeak1au.merge(peak1aulastpo[['author','created']],left_on='author',right_on='author',how='left')\n",
    "sumpeak1au.rename(columns={'created':'last_posted_in_wsb'},inplace=True)\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1augmepo = peak1auposts[(peak1auposts['title'].str.contains(\"GME|Gamestop|GameStop|GAMESTOP|gamestop|Gamestop's|gme|GameStop's\",case=False)) | (peak1auposts['body'].str.contains(\"GME|Gamestop|GameStop|GAMESTOP|gamestop|Gamestop's|gme|GameStop's\",case=False))]\n",
    "peak11stpogme = peak1augmepo.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1lastpogme = peak1augmepo.sort_values(\"created\",ascending=False).drop_duplicates(\"author\",keep='first')\n",
    "sumpeak1au = sumpeak1au.merge(peak11stpogme[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au.rename(columns={'created':'fisrt_post_gme'},inplace=True)\n",
    "sumpeak1au = sumpeak1au.merge(peak1lastpogme[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au.rename(columns={'created':'last_post_gme'},inplace=True)\n",
    "len(sumpeak1au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstpogmeinpeak1 = peak1augmepo[(peak1augmepo.created >= '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')].sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "sumpeak1au = sumpeak1au.merge(firstpogmeinpeak1[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au.rename(columns={'created':'fisrt_po_gme_inpeak1'},inplace=True)\n",
    "len(sumpeak1au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first/last time post gme, post gme frequency, average comment number, average gme post score\n",
    "gmepofreq = pd.DataFrame(peak1augmepo[(peak1augmepo.created >= '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')].groupby(['author'])['sub_id'].count())\n",
    "gmeposcore = pd.DataFrame(peak1augmepo[(peak1augmepo.created >= '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')].groupby(['author'])['score'].mean())\n",
    "gmepocomNo = pd.DataFrame(peak1augmepo[(peak1augmepo.created >= '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')].groupby(['author'])['numComms'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmeposumscore = pd.DataFrame(peak1augmepo[(peak1augmepo.created >= '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')].groupby(['author'])['score'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumpeak1au1 = sumpeak1au.merge(gmepofreq,left_on='author',right_on='author',how='inner')\n",
    "len(sumpeak1au1)\n",
    "sumpeak1au = sumpeak1au.merge(gmeposcore,left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au = sumpeak1au.merge(gmepocomNo,left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au.rename(columns={'sub_id':'No_gmepost','score':'gmepost_score','numComms':'No_gmepost_com'},inplace=True)\n",
    "sumpeak1au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au = sumpeak1au.merge(gmeposumscore,left_on='author',right_on='author',how='inner')\n",
    "sumpeak1au.rename(columns={'score':'gmepost_sumscore'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first/last time comment gme, comment gme frequency, average gme comment score\n",
    "\n",
    "peak1augmecom = peak1aucomments.merge(sumpeak1au,left_on='Author',right_on='author',how='inner')\n",
    "peak1augmecom['Reply to'] = peak1augmecom['Reply to'].apply(lambda x: x[3:])\n",
    "gme124 = pd.read_csv('gme/gme124.csv')\n",
    "gme130 = pd.read_csv('gme/gme130.csv')\n",
    "gme3713 = pd.read_csv('gme/gme3713.csv')\n",
    "gme11016 = pd.read_csv('gme/gme11016.csv')\n",
    "gme11723 = pd.read_csv('gme/gme11723.csv')\n",
    "gme12529 = pd.read_csv('gme/gme12529.csv')\n",
    "gme21420 = pd.read_csv('gme/gme21420.csv')\n",
    "gme22136 = pd.read_csv('gme/gme22136.csv')\n",
    "gme31420 = pd.read_csv('gme/gme31420.csv')\n",
    "gme131213 = pd.read_csv('gme/gme131213.csv')\n",
    "gme122119 = pd.read_csv('gme/gme12-21.csv')\n",
    "gme321531 = pd.read_csv('gme/gme321531.csv')\n",
    "allgmeposts = pd.concat([gme122119,gme11016,gme11723,gme124,gme12529,gme130,gme131213,gme21420,gme22136,gme3713,gme31420,gme321531])\n",
    "peak1augmecom = peak1augmecom.merge(allgmeposts[['Post ID','Title','Body','Flair']],left_on='Reply to',right_on='Post ID',how='left')\n",
    "peak1augmecom = peak1augmecom[(~peak1augmecom['Post ID'].isna()) | (peak1augmecom['Body_x'].str.contains(\"GME|Gamestop|GameStop|GAMESTOP|gamestop|Gamestop's|gme|GameStop's\",case=False))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstcomgmeinpeak1 = peak1augmecom[(peak1augmecom['Publish Date'] > '2021-01-25 00:00:00') & (peak1augmecom['Publish Date'] < '2021-01-30 00:00:00')].sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "sumpeak1au = sumpeak1au.merge(firstcomgmeinpeak1[['Author','Publish Date']],left_on='author',right_on='Author',how='left')\n",
    "sumpeak1au.rename(columns={'Publish Date':'fisrt_com_gme_inpeak1'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(peak1augmecom['Author'].value_counts()))\n",
    "peak11stcomgme = peak1augmecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "print(len(peak11stcomgme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1lastcomgme = peak1augmecom.sort_values(\"Publish Date\",ascending=False).drop_duplicates(\"Author\",keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au = sumpeak1au.merge(peak11stcomgme[['Author','Publish Date']],left_on='author',right_on='Author',how='left')\n",
    "sumpeak1au.rename(columns={'Publish Date':'fisrt_com_gme'},inplace=True)\n",
    "sumpeak1au = sumpeak1au.merge(peak1lastcomgme[['Author','Publish Date']],left_on='author',right_on='Author',how='left')\n",
    "sumpeak1au.rename(columns={'Publish Date':'last_com_gme'},inplace=True)\n",
    "\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sumpeak1au['Author_x']\n",
    "del sumpeak1au['Author_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmecomfreq = pd.DataFrame(peak1augmecom[(peak1augmecom['Publish Date'] > '2021-01-25 00:00:00') & (peak1augmecom['Publish Date']  < '2021-01-30 00:00:00')].groupby(['Author'])['Comment ID'].count())\n",
    "gmecomscore = pd.DataFrame(peak1augmecom[(peak1augmecom['Publish Date'] > '2021-01-25 00:00:00') & (peak1augmecom['Publish Date']  < '2021-01-30 00:00:00')].groupby(['Author'])['Score'].mean())\n",
    "sumpeak1au = sumpeak1au.merge(gmecomfreq,left_on='author',right_on='Author',how='left')\n",
    "sumpeak1au = sumpeak1au.merge(gmecomscore,left_on='author',right_on='Author',how='left')\n",
    "sumpeak1au.rename(columns={'Comment ID':'No_gmecom','Score':'gmecom_score'},inplace=True)\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first/last post gme gain/loss,frequency of posting gme gain/loss, score of gme gain/loss, No of gme gain/loss comments\n",
    "\n",
    "gmepoinpeak1 = peak1augmepo[(peak1augmepo.created > '2021-01-25 00:00:00') & (peak1augmepo.created < '2021-01-30 00:00:00')]\n",
    "\n",
    "peak1gain = gmepoinpeak1[gmepoinpeak1['flair'] == 'Gain']\n",
    "\n",
    "peak11stgain = peak1gain.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1lastgain = peak1gain.sort_values(\"created\",ascending=False).drop_duplicates(\"author\",keep='first')\n",
    "\n",
    "gainfreq = pd.DataFrame(peak1gain.groupby(['author'])['sub_id'].count())\n",
    "gainscore = pd.DataFrame(peak1gain.groupby(['author'])['score'].mean())\n",
    "gaincomNo = pd.DataFrame(peak1gain.groupby(['author'])['numComms'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au = sumpeak1au.merge(peak11stgain[['author','created']],left_on='author',right_on='author',how='left')\n",
    "sumpeak1au.rename(columns={'created':'fisrt_gain'},inplace=True)\n",
    "sumpeak1au = sumpeak1au.merge(peak1lastgain[['author','created']],left_on='author',right_on='author',how='left')\n",
    "sumpeak1au.rename(columns={'created':'last_gain'},inplace=True)\n",
    "sumpeak1au = sumpeak1au.merge(gainfreq,left_on='author',right_on='author',how='left')\n",
    "sumpeak1au = sumpeak1au.merge(gainscore,left_on='author',right_on='author',how='left')\n",
    "sumpeak1au = sumpeak1au.merge(gaincomNo,left_on='author',right_on='author',how='left')\n",
    "sumpeak1au.rename(columns={'sub_id':'No_gainpost','score':'gain_score','numComms':'No_gain_com'},inplace=True)\n",
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First in wsb / First mention gme\n",
    "sumpeak1au['first_in_wsb'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if str(sumpeak1au['first_commented_in_wsb'][i]) == 'NaT':\n",
    "        sumpeak1au['first_in_wsb'][i] = sumpeak1au['first_posted_in_wsb'][i]\n",
    "    else:\n",
    "        if sumpeak1au['first_posted_in_wsb'][i] < sumpeak1au['first_commented_in_wsb'][i]:\n",
    "            sumpeak1au['first_in_wsb'][i] = sumpeak1au['first_posted_in_wsb'][i]\n",
    "        else:\n",
    "            sumpeak1au['first_in_wsb'][i] = sumpeak1au['first_commented_in_wsb'][i]\n",
    "\n",
    "sumpeak1au['last_in_wsb'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if str(sumpeak1au['last_commented_in_wsb'][i]) == 'NaT':\n",
    "        sumpeak1au['last_in_wsb'][i] = sumpeak1au['last_posted_in_wsb'][i]\n",
    "    else:\n",
    "        if sumpeak1au['last_posted_in_wsb'][i] < sumpeak1au['last_commented_in_wsb'][i]:\n",
    "            sumpeak1au['last_in_wsb'][i] = sumpeak1au['last_commented_in_wsb'][i]\n",
    "        else:\n",
    "            sumpeak1au['last_in_wsb'][i] = sumpeak1au['last_posted_in_wsb'][i]\n",
    "\n",
    "sumpeak1au['first_gme'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if str(sumpeak1au['fisrt_com_gme'][i]) == 'NaT':\n",
    "        sumpeak1au['first_gme'][i] = sumpeak1au['fisrt_post_gme'][i]\n",
    "    else:\n",
    "        if sumpeak1au['fisrt_post_gme'][i] < sumpeak1au['fisrt_com_gme'][i]:\n",
    "            sumpeak1au['first_gme'][i] = sumpeak1au['fisrt_post_gme'][i]\n",
    "        else:\n",
    "            sumpeak1au['first_gme'][i] = sumpeak1au['fisrt_com_gme'][i]\n",
    "\n",
    "sumpeak1au['last_gme'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if str(sumpeak1au['last_com_gme'][i]) == 'NaT':\n",
    "        sumpeak1au['last_gme'][i] = sumpeak1au['last_post_gme'][i]\n",
    "    else:\n",
    "        if sumpeak1au['last_post_gme'][i] < sumpeak1au['last_com_gme'][i]:\n",
    "            sumpeak1au['last_gme'][i] = sumpeak1au['last_com_gme'][i]\n",
    "        else:\n",
    "            sumpeak1au['last_gme'][i] = sumpeak1au['last_post_gme'][i]\n",
    "\n",
    "sumpeak1au['first_gme_inpeak1'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if str(sumpeak1au['fisrt_com_gme_inpeak1'][i]) == 'NaT':\n",
    "        sumpeak1au['first_gme_inpeak1'][i] = sumpeak1au['fisrt_po_gme_inpeak1'][i]\n",
    "    else:\n",
    "        if sumpeak1au['fisrt_po_gme_inpeak1'][i] < sumpeak1au['fisrt_com_gme_inpeak1'][i]:\n",
    "            sumpeak1au['first_gme_inpeak1'][i] = sumpeak1au['fisrt_po_gme_inpeak1'][i]\n",
    "        else:\n",
    "            sumpeak1au['first_gme_inpeak1'][i] = sumpeak1au['fisrt_com_gme_inpeak1'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# categorize pre-gme / peak1 users (keyline: Jan 25th)\n",
    "# pre-gme=0 peak1 = 1\n",
    "sumpeak1au['first_cate'] = ''\n",
    "for i,r in sumpeak1au.iterrows():\n",
    "    if sumpeak1au['first_in_wsb'][i] < datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0):\n",
    "        sumpeak1au['first_cate'][i] = 0\n",
    "    else:\n",
    "        sumpeak1au['first_cate'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New vs. Old\n",
    "print(len(sumpeak1au[sumpeak1au['first_cate'] == 1])/29944)\n",
    "\n",
    "print(len(sumpeak1au[sumpeak1au['first_posted_in_wsb'] > '2021-01-25 00:00:00']))\n",
    "print(len(sumpeak1au[sumpeak1au['first_posted_in_wsb'] > '2021-01-25 00:00:00'])/29944)\n",
    "\n",
    "print(len(sumpeak1au[sumpeak1au['first_commented_in_wsb'] > '2021-01-25 00:00:00']))\n",
    "print(len(sumpeak1au[sumpeak1au['first_commented_in_wsb'] > '2021-01-25 00:00:00'])/29944)\n",
    "\n",
    "print(len(sumpeak1au[sumpeak1au['first_in_wsb'] > datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)]))\n",
    "print(len(sumpeak1au[sumpeak1au['first_in_wsb'] > datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)])/29944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpeak1au1stpo = sumpeak1au.set_index(\"first_in_wsb\")\n",
    "tpeak1au1stpo.groupby(pd.Grouper(freq='M'))['author'].count().plot(label='number of authors')\n",
    "plt.legend()\n",
    "plt.savefig('/Users/elaine/Desktop/firstinwsb.png',bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['post_length']=(datetime(year=2021, month=1, day=30, hour=23, minute=59, second=59)-sumpeak1au['first_posted_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['comment_length']=(datetime(year=2021, month=1, day=30, hour=23, minute=59, second=59)-sumpeak1au['first_commented_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['content_length']=(datetime(year=2021, month=1, day=30, hour=23, minute=59, second=59)-sumpeak1au['first_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['post_length'].mean())\n",
    "print(sumpeak1au['post_length'].median())\n",
    "print(sumpeak1au['post_length'].std())\n",
    "\n",
    "print(sumpeak1au['comment_length'].mean())\n",
    "print(sumpeak1au['comment_length'].median())\n",
    "print(sumpeak1au['comment_length'].std())\n",
    "\n",
    "print(sumpeak1au['content_length'].mean())\n",
    "print(sumpeak1au['content_length'].median())\n",
    "print(sumpeak1au['content_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when they first post gme\n",
    "sumpeak1au['firstgme_length']=(datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0)-sumpeak1au['first_gme']).apply(lambda x: x.days)\n",
    "print(sumpeak1au['firstgme_length'].mean())\n",
    "print(sumpeak1au['firstgme_length'].median())\n",
    "print(sumpeak1au['firstgme_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfg = sumpeak1au[sumpeak1au['first_gme'] > datetime(year=2020, month=1, day=1, hour=0, minute=0, second=0)]\n",
    "tfg = tfg.set_index('first_gme')\n",
    "tfg.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week after 2020')\n",
    "#plt.xlim(left=2021)\n",
    "plt.legend()\n",
    "plt.savefig('/Users/elaine/Desktop/firstgme.png',bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1newcomers = sumpeak1au[sumpeak1au['first_cate'] == 1]\n",
    "print(len(peak1newcomers))\n",
    "peak1oldmembers = sumpeak1au[sumpeak1au['first_cate'] == 0]\n",
    "print(len(peak1oldmembers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1newcomers[peak1newcomers['last_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1newcomers[peak1newcomers['last_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/19660)\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/10284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1newcomers[peak1newcomers['last_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1newcomers[peak1newcomers['last_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/19660)\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/10284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1oldmembers[(peak1oldmembers['first_gme'] > datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)) & (peak1oldmembers['last_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2633/10284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#never wsb after 1.29\n",
    "print((10522+2263)/29944)\n",
    "#never GME after 1.29\n",
    "print((13710+3785)/29944)\n",
    "#only GME 1.25-29\n",
    "print((13710+2633)/29944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#单看下post\n",
    "print(len(peak1newcomers[peak1newcomers['last_posted_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1newcomers[peak1newcomers['last_posted_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/19660)\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_posted_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_posted_in_wsb'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/10284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1newcomers[peak1newcomers['last_post_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1newcomers[peak1newcomers['last_post_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/19660)\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_post_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]))\n",
    "print(len(peak1oldmembers[peak1oldmembers['last_post_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)])/10284)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1oldmembers[(peak1oldmembers['fisrt_post_gme'] > datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)) & (peak1oldmembers['last_post_gme'] < datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7154/10284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# care GME after 129\n",
    "peak1augme129 = sumpeak1au[sumpeak1au['last_gme'] >= datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0)]\n",
    "len(peak1augme129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1augme129['care_length'] = (peak1augme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "print(peak1augme129['care_length'].mean())\n",
    "print(peak1augme129['care_length'].median())\n",
    "print(peak1augme129['care_length'].std())\n",
    "print(peak1augme129['care_length'].min())\n",
    "print(peak1augme129['care_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1augme129['wsb_length'] = (peak1augme129['last_in_wsb'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "print(peak1augme129['wsb_length'].mean())\n",
    "print(peak1augme129['wsb_length'].median())\n",
    "print(peak1augme129['wsb_length'].std())\n",
    "print(peak1augme129['wsb_length'].min())\n",
    "print(peak1augme129['wsb_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['care_length'].mean())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['care_length'].median())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['care_length'].std())\n",
    "\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['care_length'].mean())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['care_length'].median())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['care_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['wsb_length'].mean())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['wsb_length'].median())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 0]['wsb_length'].std())\n",
    "\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['wsb_length'].mean())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['wsb_length'].median())\n",
    "print(peak1augme129[peak1augme129['first_cate'] == 1]['wsb_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1augme129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1augme129['first_cate']\n",
    "x1= peak1augme129['care_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/math.exp(-0.0121)\n",
    "# Users who concerned GME for longer time are 1.012 times more likely to be old members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1augme129['first_cate']\n",
    "x1= peak1augme129['wsb_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/math.exp(-0.0141)\n",
    "# Users who stayed in wsb for longer time are 1.014 times more likely to be old members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1gain = pd.DataFrame(peak1gain['author'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1gain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1gaingme = pd.merge(peak1gain,sumpeak1au[['author','last_gme','last_in_wsb','first_cate']],left_index=True,right_on='author',how='inner')\n",
    "print(len(peak1gaingme))\n",
    "print(len(peak1gaingme[peak1gaingme['last_gme'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1gaingme[peak1gaingme['last_gme'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/2092)\n",
    "print(len(peak1gaingme[peak1gaingme['last_in_wsb'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1gaingme[peak1gaingme['last_in_wsb'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/2092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1gaingme129 = peak1gaingme[peak1gaingme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]\n",
    "print(len(peak1gaingme129))\n",
    "peak1gaingme129['care_length'] = (peak1gaingme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "peak1gaingme129['wsb_length'] = (peak1gaingme129['last_in_wsb'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "print(peak1gaingme129['care_length'].mean())\n",
    "print(peak1gaingme129['care_length'].median())\n",
    "print(peak1gaingme129['care_length'].std())\n",
    "print(peak1gaingme129['wsb_length'].mean())\n",
    "print(peak1gaingme129['wsb_length'].median())\n",
    "print(peak1gaingme129['wsb_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['care_length'].mean())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['care_length'].median())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['care_length'].std())\n",
    "\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['care_length'].mean())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['care_length'].median())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['care_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['wsb_length'].mean())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['wsb_length'].median())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 0]['wsb_length'].std())\n",
    "\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['wsb_length'].mean())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['wsb_length'].median())\n",
    "print(peak1gaingme129[peak1gaingme129['first_cate'] == 1]['wsb_length'].std())## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1gaingme129['first_cate']\n",
    "x1= peak1gaingme129['care_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/math.exp(-0.0153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1gaingme129['first_cate']\n",
    "x1= peak1gaingme129['wsb_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yolo = peak1augmepo[peak1augmepo['flair'] == 'YOLO']\n",
    "peak11yolo = peak1yolo[(peak1yolo['created'] > '2021-01-25 00:00:00') & (peak1yolo['created'] < '2021-01-30 00:00:00')]['author'].value_counts()\n",
    "peak11yolo = pd.DataFrame(peak11yolo)\n",
    "len(peak11yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yologme = pd.merge(peak11yolo,sumpeak1au[['author','last_gme','last_in_wsb','first_cate']],left_index=True,right_on='author',how='inner')\n",
    "len(peak1yologme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1yologme[peak1yologme['last_gme'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1yologme[peak1yologme['last_gme'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/6636)\n",
    "print(len(peak1yologme[peak1yologme['last_in_wsb'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1yologme[peak1yologme['last_in_wsb'] <= datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/6636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yologme129 = peak1yologme[peak1yologme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]\n",
    "print(len(peak1yologme129))\n",
    "peak1yologme129['care_length'] = (peak1yologme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "peak1yologme129['wsb_length'] = (peak1yologme129['last_in_wsb'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n",
    "print(peak1yologme129['care_length'].mean())\n",
    "print(peak1yologme129['care_length'].median())\n",
    "print(peak1yologme129['care_length'].std())\n",
    "print(peak1yologme129['wsb_length'].mean())\n",
    "print(peak1yologme129['wsb_length'].median())\n",
    "print(peak1yologme129['wsb_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['care_length'].mean())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['care_length'].median())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['care_length'].std())\n",
    "\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['care_length'].mean())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['care_length'].median())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['care_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['wsb_length'].mean())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['wsb_length'].median())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 0]['wsb_length'].std())\n",
    "\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['wsb_length'].mean())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['wsb_length'].median())\n",
    "print(peak1yologme129[peak1yologme129['first_cate'] == 1]['wsb_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1yologme129['first_cate']\n",
    "x1= peak1yologme129['care_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = peak1yologme129['first_cate']\n",
    "x1= peak1yologme129['wsb_length']\n",
    "\n",
    "x = sm.add_constant(x1)\n",
    "x.shape\n",
    "\n",
    "results_log = sm.Logit(y.astype(float),x.astype(float)).fit()\n",
    "print(results_log.summary())\n",
    "\n",
    "cm_df = pd.DataFrame(results_log.pred_table())\n",
    "cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n",
    "display(cm_df)\n",
    "\n",
    "# Model Accuracy\n",
    "cm = np.array(cm_df)\n",
    "accuracy_train = 100*(cm[0,0]+cm[1,1])/cm.sum()\n",
    "print ('The model accuracy based is {:.5}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(-0.0118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.29前发过内容，1.30-3.7之间没有，3.8-3.12之间发过内容\n",
    "peak1peak2 = pd.merge(peak1augmepo[['author','created']],sumpeak1au[['author','first_cate']],left_on='author',right_on='author',how='inner')\n",
    "len(peak1peak2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1peak2 = peak1peak2.merge(peak1augmecom[['Author','Publish Date']],left_on='author',right_on='Author',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.30-3.7发过GME言的人\n",
    "peak12po = peak1peak2[(peak1peak2['created'] > '2021-01-29 23:59:59') & (peak1peak2['created'] <= '2021-03-07 23:59:59')]['author'].value_counts()\n",
    "peak12com = peak1peak2[(peak1peak2['Publish Date'] > '2021-01-29 23:59:59') & (peak1peak2['Publish Date'] <= '2021-03-07 23:59:59')]['author'].value_counts()\n",
    "def Union(lst1, lst2):\n",
    "    final_list = list(set(lst1) | set(lst2))\n",
    "    return final_list\n",
    "peak12au13037 = Union(peak12po.index.tolist(),peak12com.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.8-3.12发过GME言的人\n",
    "peak12po3812 = peak1peak2[(peak1peak2['created'] > '2021-03-07 23:59:59') & (peak1peak2['created'] <= '2021-03-12 23:59:59')]['author'].value_counts()\n",
    "peak12com3812 = peak1peak2[(peak1peak2['Publish Date'] > '2021-03-07 23:59:59') & (peak1peak2['Publish Date'] <= '2021-03-12 23:59:59')]['author'].value_counts()\n",
    "def Union(lst1, lst2):\n",
    "    final_list = list(set(lst1) | set(lst2))\n",
    "    return final_list\n",
    "peak12au3812 = Union(peak12po3812.index.tolist(),peak12com3812.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak12au = []\n",
    "for i in peak12au3812:\n",
    "    if i in peak12au13037:\n",
    "        continue\n",
    "    else:\n",
    "        peak12au.append(i)\n",
    "len(peak12au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "304/12449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak12au = pd.DataFrame(peak12au,columns=['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak12au = peak12au.merge(sumpeak1au[['author','last_gme','last_in_wsb','first_cate']],left_on='author', right_on='author',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>last_gme</th>\n",
       "      <th>last_in_wsb</th>\n",
       "      <th>first_cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kevlen123</td>\n",
       "      <td>2021-03-11 07:08:29</td>\n",
       "      <td>2021-03-11 07:08:29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nahsor107</td>\n",
       "      <td>2021-04-01 05:11:57</td>\n",
       "      <td>2021-04-01 05:11:57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tophurkey</td>\n",
       "      <td>2021-04-14 23:27:05</td>\n",
       "      <td>2021-04-14 23:27:05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mju516</td>\n",
       "      <td>2021-04-16 22:32:23</td>\n",
       "      <td>2021-04-16 22:32:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jbronnier</td>\n",
       "      <td>2021-03-09 01:42:39</td>\n",
       "      <td>2021-03-09 01:42:39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>ThatBoiZach</td>\n",
       "      <td>2021-03-11 02:06:01</td>\n",
       "      <td>2021-03-11 02:06:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>ijustwant2feelbetter</td>\n",
       "      <td>2021-03-17 12:21:42</td>\n",
       "      <td>2021-03-17 12:21:42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>thebirdsnthebeemovie</td>\n",
       "      <td>2021-03-09 05:07:07</td>\n",
       "      <td>2021-03-09 05:07:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>NYJets18</td>\n",
       "      <td>2021-03-11 01:43:59</td>\n",
       "      <td>2021-03-11 01:43:59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>happy-facade</td>\n",
       "      <td>2021-03-09 20:33:59</td>\n",
       "      <td>2021-05-17 08:54:56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author             last_gme          last_in_wsb first_cate\n",
       "0               kevlen123  2021-03-11 07:08:29  2021-03-11 07:08:29          1\n",
       "1               Nahsor107  2021-04-01 05:11:57  2021-04-01 05:11:57          1\n",
       "2               Tophurkey  2021-04-14 23:27:05  2021-04-14 23:27:05          1\n",
       "4                  mju516  2021-04-16 22:32:23  2021-04-16 22:32:23          1\n",
       "5               jbronnier  2021-03-09 01:42:39  2021-03-09 01:42:39          1\n",
       "..                    ...                  ...                  ...        ...\n",
       "296           ThatBoiZach  2021-03-11 02:06:01  2021-03-11 02:06:01          1\n",
       "298  ijustwant2feelbetter  2021-03-17 12:21:42  2021-03-17 12:21:42          1\n",
       "299  thebirdsnthebeemovie  2021-03-09 05:07:07  2021-03-09 05:07:07          1\n",
       "300              NYJets18  2021-03-11 01:43:59  2021-03-11 01:43:59          1\n",
       "301          happy-facade  2021-03-09 20:33:59  2021-05-17 08:54:56          1\n",
       "\n",
       "[173 rows x 4 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak12au[peak12au['first_cate'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used mooon in GME content 次数 / 第一次的时间 / ave upvotes(post+comment 总数/总数量)\n",
    "moongmepost = peak1augmepo[(peak1augmepo.body.str.contains('moon|Moon|MOON',case=False)) | (peak1augmepo.title.str.contains('moon|Moon|MOON',case=False))]\n",
    "moongmecom = peak1augmecom[(~peak1augmecom.Body_x.isna()) & (peak1augmecom.Body_x.str.contains('moon|Moon|MOON',case=False))]\n",
    "moongmepost = moongmepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "moongmecom = moongmecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del moongmecom['author']\n",
    "moongmecomfreq = pd.DataFrame(moongmecom.groupby(['Author'])['Comment ID'].count())\n",
    "moongmecomfreq.rename(columns={'Comment ID':'No_gmecom_moon'},inplace=True)\n",
    "moongmepofreq = pd.DataFrame(moongmepost.groupby(['author'])['sub_id'].count())\n",
    "moongmepofreq.rename(columns={'sub_id':'No_gmepo_moon'},inplace=True)\n",
    "moongmeplotpo = moongmepost[['created','sub_id']]\n",
    "moongmeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "moongmeplotcom = moongmecom[['Publish Date','Comment ID']]\n",
    "moongmeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "moongmeplot = pd.concat([moongmeplotpo,moongmeplotcom])\n",
    "print(len(moongmeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au[sumpeak1au['first_gme'] > datetime(year=2021, month=1, day=10, hour=0, minute=0, second=0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "123-(31+30+31+28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.ols(formula='firstgme_length ~ truecontent_length', data=sumpeak1au).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first and last gme post - when they step out\n",
    "sumpeak1au['gmepost_length']=(sumpeak1au['last_post_gme']-sumpeak1au['fisrt_post_gme']).apply(lambda x: x.days)\n",
    "print(sumpeak1au['gmepost_length'].mean())\n",
    "print(sumpeak1au['gmepost_length'].median())\n",
    "print(sumpeak1au['gmepost_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(sumpeak1au['gmepost_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au[(sumpeak1au['fisrt_post_gme']>'2021-01-25 00:00:00') & (sumpeak1au[\"last_post_gme\"]<'2021-01-30 00:00:00')]))\n",
    "print(len(sumpeak1au[(sumpeak1au['fisrt_post_gme']>'2021-01-25 00:00:00') & (sumpeak1au[\"last_post_gme\"]<'2021-01-30 00:00:00')])/30013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au[~sumpeak1au['fisrt_com_gme'].isna()]))\n",
    "print(len(sumpeak1au[~sumpeak1au['fisrt_com_gme'].isna()])/30013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['gmecom_length']=(sumpeak1au['last_com_gme']-sumpeak1au['fisrt_com_gme']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['gmecom_length'].mean())\n",
    "print(sumpeak1au['gmecom_length'].median())\n",
    "print(sumpeak1au['gmecom_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(sumpeak1au[~sumpeak1au['gmecom_length'].isna()]['gmecom_length'], 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au[(sumpeak1au['fisrt_com_gme']>'2021-01-25 00:00:00') & (sumpeak1au[\"last_com_gme\"]<'2021-01-30 00:00:00')]))\n",
    "print(len(sumpeak1au[(sumpeak1au['fisrt_com_gme']>'2021-01-25 00:00:00') & (sumpeak1au[\"last_com_gme\"]<'2021-01-30 00:00:00')])/17112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['gme_length']=(sumpeak1au['last_gme']-sumpeak1au['first_gme']).apply(lambda x: x.days)\n",
    "print(sumpeak1au['gme_length'].mean())\n",
    "print(sumpeak1au['gme_length']..median())\n",
    "print(sumpeak1au['gme_length'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.percentile(sumpeak1au['gme_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sumpeak1au[(sumpeak1au['first_gme']>datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)) & (sumpeak1au[\"last_gme\"]<datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0))]))\n",
    "print(len(sumpeak1au[(sumpeak1au['first_gme']>datetime(year=2021, month=1, day=25, hour=0, minute=0, second=0)) & (sumpeak1au[\"last_gme\"]<datetime(year=2021, month=1, day=30, hour=0, minute=0, second=0))])/30013)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people who still cared GME after 1.29\n",
    "\n",
    "print(len(sumpeak1au[sumpeak1au['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(sumpeak1au[sumpeak1au['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/30013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1augme129 = sumpeak1au[sumpeak1au['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]\n",
    "peak1augme129['care_length'] = (peak1augme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1augme129['care_length'].value_counts().plot.bar(label='number of users')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1augme129['care_length'].mean())\n",
    "print(peak1augme129['care_length'].median())\n",
    "print(peak1augme129['care_length'].std())\n",
    "print(peak1augme129['care_length'].min())\n",
    "print(peak1augme129['care_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(peak1augme129['care_length'], 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time cared GME who Gain/YOLO during the first peak\n",
    "peak11gain = peak1gain[(peak1gain['created'] > '2021-01-25 00:00:00') & (peak1gain['created'] < '2021-01-30 00:00:00')]['author'].value_counts()\n",
    "peak11gain = pd.DataFrame(peak11gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak11gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1gaingme = pd.merge(peak11gain,sumpeak1au[['author','last_gme','first_in_wsb']],left_index=True,right_on='author',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1gaingme[peak1gaingme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1gaingme[peak1gaingme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/2092)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1gaingme129 = peak1gaingme[peak1gaingme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]\n",
    "peak1gaingme129['care_length'] = (peak1gaingme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1gaingme129['care_length'].value_counts().plot.bar(label='number of users')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1gaingme129['care_length'].mean())\n",
    "print(peak1gaingme129['care_length'].median())\n",
    "print(peak1gaingme129['care_length'].std())\n",
    "print(peak1gaingme129['care_length'].min())\n",
    "print(peak1gaingme129['care_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yolo = peak1augmepo[peak1augmepo['flair'] == 'YOLO']\n",
    "peak11yolo = peak1yolo[(peak1yolo['created'] > '2021-01-25 00:00:00') & (peak1yolo['created'] < '2021-01-30 00:00:00')]['author'].value_counts()\n",
    "peak11yolo = pd.DataFrame(peak11yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak11yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yologme = pd.merge(peak11yolo,sumpeak1au[['author','last_gme','first_in_wsb']],left_index=True,right_on='author',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(peak1yologme[peak1yologme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]))\n",
    "print(len(peak1yologme[peak1yologme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)])/6636)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1yologme129 = peak1yologme[peak1yologme['last_gme'] > datetime(year=2021, month=1, day=29, hour=23, minute=59, second=59)]\n",
    "peak1yologme129['care_length'] = (peak1yologme129['last_gme'] - datetime(year=2021, month=1, day=29, hour=0, minute=0, second=0)).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peak1yologme129['care_length'].mean())\n",
    "print(peak1yologme129['care_length'].median())\n",
    "print(peak1yologme129['care_length'].std())\n",
    "print(peak1yologme129['care_length'].min())\n",
    "print(peak1yologme129['care_length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score,frequency, no_com time relationship (5.31 is last day for collection)\n",
    "print(sumpeak1au['gmepost_score'].mean())\n",
    "print(sumpeak1au['gmepost_score'].median())\n",
    "print(sumpeak1au['gmepost_score'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['No_gmepost_com'].mean())\n",
    "print(sumpeak1au['No_gmepost_com'].median())\n",
    "print(sumpeak1au['No_gmepost_com'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['No_gmepost'].mean())\n",
    "print(sumpeak1au['No_gmepost'].median())\n",
    "print(sumpeak1au['No_gmepost'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['No_gmepost'].value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,j in sumpeak1au['No_gmecom'].iteritems():\n",
    "    if str(sumpeak1au['No_gmecom'][i]) == 'nan':\n",
    "        sumpeak1au['No_gmecom'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['No_gme'] = sumpeak1au['No_gmecom'] + sumpeak1au['No_gmepost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['No_gmecom'].mean())\n",
    "print(sumpeak1au['No_gmecom'].median())\n",
    "print(sumpeak1au['No_gmecom'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumpeak1au['No_gme'].mean())\n",
    "print(sumpeak1au['No_gme'].median())\n",
    "print(sumpeak1au['No_gme'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au['truecontent_length']=(datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0)-sumpeak1au['first_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au.to_csv('/Users/elaine/Desktop/sumdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = sm.ols(formula='gmecom_score ~ truecontent_length', data=sumpeak1au).fit()\n",
    "print(result1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(sumpeak1au['gmepost_score'],sumpeak1au['No_gme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay time relationship after 1.29\n",
    "result2 = sm.ols(formula='gmecom_length ~ truecontent_length', data=sumpeak1au).fit()\n",
    "print(result2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1augme129['truecontent_length']=(datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0)-peak1augme129['first_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = sm.ols(formula='care_length ~ truecontent_length', data=peak1augme129).fit()\n",
    "print(result3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1gaingme129['truecontent_length']=(datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0)-peak1gaingme129['first_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = sm.ols(formula='care_length ~ truecontent_length', data=peak1gaingme129).fit()\n",
    "print(result4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1yologme129['truecontent_length']=(datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0)-peak1yologme129['first_in_wsb']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = sm.ols(formula='care_length ~ truecontent_length', data=peak1yologme129).fit()\n",
    "print(result5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cultural form - meme/YOLO/News\n",
    "peak1meme = peak1augmepo[peak1augmepo['flair'] == 'Meme']\n",
    "peak11stmeme = peak1meme.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1memefreq = pd.DataFrame(peak1meme.groupby(['author'])['sub_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peak1culmeme = pd.merge(peak1memefreq,sumpeak1au[['author','last_gme','first_in_wsb']],left_on='author',right_on='author',how='inner')\n",
    "peak1culmeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1culmeme = peak1culmeme.merge(peak11stmeme[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "peak1culmeme.rename(columns={'created':'fist_gme_meme','sub_id':'No_gme_meme'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1culmeme['content_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culmeme['first_in_wsb']).apply(lambda x: x.days)\n",
    "peak1culmeme['meme_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culmeme['fist_gme_meme']).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1culmeme)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = sm.ols(formula='meme_length ~ content_length', data=peak1culmeme).fit()\n",
    "print(result6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = sm.ols(formula='No_gme_meme ~ content_length', data=peak1culmeme).fit()\n",
    "print(result6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1yolo = peak1augmepo[peak1augmepo['flair'] == 'YOLO']\n",
    "peak11styolo = peak1yolo.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1yolofreq = pd.DataFrame(peak1yolo.groupby(['author'])['sub_id'].count())\n",
    "peak1culyolo = pd.merge(peak1yolofreq,sumpeak1au[['author','last_gme','first_in_wsb']],left_on='author',right_on='author',how='inner')\n",
    "peak1culyolo = peak1culyolo.merge(peak11styolo[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "peak1culyolo.rename(columns={'created':'fist_gme_yolo','sub_id':'No_gme_yolo'},inplace=True)\n",
    "peak1culyolo['content_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culyolo['first_in_wsb']).apply(lambda x: x.days)\n",
    "peak1culyolo['yolo_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culyolo['fist_gme_yolo']).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1culyolo)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = sm.ols(formula='yolo_length ~ content_length', data=peak1culyolo).fit()\n",
    "print(result6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = sm.ols(formula='No_gme_yolo ~ content_length', data=peak1culyolo).fit()\n",
    "print(result6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1news = peak1augmepo[peak1augmepo['flair'] == 'News']\n",
    "peak11stnews = peak1news.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1newsfreq = pd.DataFrame(peak1news.groupby(['author'])['sub_id'].count())\n",
    "peak1culnews = pd.merge(peak1newsfreq,sumpeak1au[['author','last_gme','first_in_wsb']],left_on='author',right_on='author',how='inner')\n",
    "peak1culnews = peak1culnews.merge(peak11stnews[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "peak1culnews.rename(columns={'created':'fist_gme_news','sub_id':'No_gme_news'},inplace=True)\n",
    "peak1culnews['content_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culnews['first_in_wsb']).apply(lambda x: x.days)\n",
    "peak1culnews['news_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culnews['fist_gme_news']).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1culnews)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result7 = sm.ols(formula='news_length ~ content_length', data=peak1culnews).fit()\n",
    "print(result7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result7 = sm.ols(formula='No_gme_news ~ content_length', data=peak1culnews).fit()\n",
    "print(result7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1ana = peak1augmepo[(peak1augmepo['flair'] == 'DD') | (peak1augmepo['flair'] == 'Technical Analysis')]\n",
    "peak11stana = peak1ana.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1anafreq = pd.DataFrame(peak1ana.groupby(['author'])['sub_id'].count())\n",
    "peak1culana = pd.merge(peak1anafreq,sumpeak1au[['author','last_gme','first_in_wsb']],left_on='author',right_on='author',how='inner')\n",
    "peak1culana = peak1culana.merge(peak11stana[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "peak1culana.rename(columns={'created':'fist_gme_ana','sub_id':'No_gme_ana'},inplace=True)\n",
    "peak1culana['content_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culana['first_in_wsb']).apply(lambda x: x.days)\n",
    "peak1culana['ana_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culana['fist_gme_ana']).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1culana)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result8 = sm.ols(formula='ana_length ~ content_length', data=peak1culana).fit()\n",
    "print(result8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result8 = sm.ols(formula='No_gme_ana ~ content_length', data=peak1culana).fit()\n",
    "print(result8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1dis = peak1augmepo[peak1augmepo['flair'] == 'Discussion']\n",
    "peak11stdis = peak1dis.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "peak1disfreq = pd.DataFrame(peak1dis.groupby(['author'])['sub_id'].count())\n",
    "peak1culdis = pd.merge(peak1disfreq,sumpeak1au[['author','last_gme','first_in_wsb']],left_on='author',right_on='author',how='inner')\n",
    "peak1culdis = peak1culdis.merge(peak11stdis[['author','created']],left_on='author',right_on='author',how='inner')\n",
    "peak1culdis.rename(columns={'created':'fist_gme_dis','sub_id':'No_gme_dis'},inplace=True)\n",
    "peak1culdis['content_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culdis['first_in_wsb']).apply(lambda x: x.days)\n",
    "peak1culdis['dis_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - peak1culdis['fist_gme_dis']).apply(lambda x: x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(peak1culdis)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result9 = sm.ols(formula='dis_length ~ content_length', data=peak1culdis).fit()\n",
    "print(result9.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result9 = sm.ols(formula='No_gme_dis ~ content_length', data=peak1culdis).fit()\n",
    "print(result9.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# critical discourse\n",
    "moonpost = peak1auposts[(peak1auposts.body.str.contains('moon|Moon|MOON',case=False)) | (peak1auposts.title.str.contains('moon|Moon|MOON',case=False))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mooncom = peak1aucomments[(~peak1aucomments.Body.isna()) & (peak1aucomments.Body.str.contains('moon|Moon|MOON',case=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonpost = moonpost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mooncom = mooncom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del mooncom['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mooncomfreq = pd.DataFrame(mooncom.groupby(['Author'])['Comment ID'].count())\n",
    "mooncomfreq.rename(columns={'Comment ID':'No_com_moon'},inplace=True)\n",
    "moonpofreq = pd.DataFrame(moonpost.groupby(['author'])['sub_id'].count())\n",
    "moonpofreq.rename(columns={'sub_id':'No_po_moon'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moonplotpo = moonpost[['created','sub_id']]\n",
    "moonplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "moonplotcom = mooncom[['Publish Date','Comment ID']]\n",
    "moonplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonplot = pd.concat([moonplotpo,moonplotcom])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times moon has been used?\n",
    "len(moonplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmoonplot = moonplot.set_index('created')\n",
    "tmoonplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'moon' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first post moon time\n",
    "moonpost = moonpost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "moonpost.rename(columns={'created':'first_po_moon'},inplace=True)\n",
    "# get first comment moon time\n",
    "mooncom = mooncom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "mooncom.rename(columns={'Publish Date':'first_com_moon','Author':'author'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moonpiece = pd.merge(moonpost,mooncom,left_on='author',right_on='author',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonpiece = pd.merge(moonpiece[['author','first_po_moon','first_com_moon']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moonpiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonpiece = moonpiece.merge(mooncomfreq,left_on='author',right_on='Author',how='left')\n",
    "len(moonpiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonpiece = moonpiece.merge(moonpofreq,left_on='author',right_on='author',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,j in moonpiece['No_com_moon'].iteritems():\n",
    "    if str(moonpiece['No_com_moon'][i]) == 'nan':\n",
    "        moonpiece['No_com_moon'][i] = 0\n",
    "for i,j in moonpiece['No_po_moon'].iteritems():\n",
    "    if str(moonpiece['No_po_moon'][i]) == 'nan':\n",
    "        moonpiece['No_po_moon'][i] = 0\n",
    "moonpiece['No_moon'] = moonpiece['No_po_moon'] + moonpiece['No_com_moon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moonpiece['first_moon']=''\n",
    "for i,r in moonpiece.iterrows():\n",
    "    if str(moonpiece['first_po_moon'][i]) == 'NaT':\n",
    "        moonpiece['first_moon'][i] = moonpiece['first_com_moon'][i]\n",
    "    else:\n",
    "        if str(moonpiece['first_com_moon'][i]) == 'NaT':\n",
    "            moonpiece['first_moon'][i] = moonpiece['first_po_moon'][i]\n",
    "        else:\n",
    "            if moonpiece['first_po_moon'][i] < moonpiece['first_com_moon'][i]:\n",
    "                moonpiece['first_moon'][i] = moonpiece['first_po_moon'][i]\n",
    "            else:\n",
    "                moonpiece['first_moon'][i] = moonpiece['first_com_moon'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = moonpiece.set_index('first_moon')\n",
    "tfm.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonpiece['firstmoon_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - moonpiece['first_moon']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstmoon_length ~ truecontent_length', data=moonpiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='No_moon ~ truecontent_length', data=moonpiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='gmepost_score ~ firstmoon_length', data=moonpiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use moon before first gme\n",
    "len(moonpiece[moonpiece['first_gme'] > moonpiece['first_moon']])/8135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use moon at or after first gme\n",
    "len(moonpiece[moonpiece['first_gme'] <= moonpiece['first_moon']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6151/8135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonaftergme = moonpiece[moonpiece['first_gme'] <= moonpiece['first_moon']]\n",
    "tmoonaftergme = moonaftergme.set_index('first_in_wsb')\n",
    "tmoonaftergme.groupby(pd.Grouper(freq='W'))['author'].count().plot(label=\"number of authors used 'moon' after GME by week\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmoonaftergme.groupby(pd.Grouper(freq='W'))['author'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3136/len(moonaftergme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstmoon_length ~ truecontent_length', data=moonaftergme).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used mooon in GME content\n",
    "moongmepost = peak1augmepo[(peak1augmepo.body.str.contains('moon|Moon|MOON',case=False)) | (peak1augmepo.title.str.contains('moon|Moon|MOON',case=False))]\n",
    "moongmecom = peak1augmecom[(~peak1augmecom.Body_x.isna()) & (peak1augmecom.Body_x.str.contains('moon|Moon|MOON',case=False))]\n",
    "moongmepost = moongmepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "moongmecom = moongmecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del moongmecom['author']\n",
    "moongmecomfreq = pd.DataFrame(moongmecom.groupby(['Author'])['Comment ID'].count())\n",
    "moongmecomfreq.rename(columns={'Comment ID':'No_gmecom_moon'},inplace=True)\n",
    "moongmepofreq = pd.DataFrame(moongmepost.groupby(['author'])['sub_id'].count())\n",
    "moongmepofreq.rename(columns={'sub_id':'No_gmepo_moon'},inplace=True)\n",
    "moongmeplotpo = moongmepost[['created','sub_id']]\n",
    "moongmeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "moongmeplotcom = moongmecom[['Publish Date','Comment ID']]\n",
    "moongmeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "moongmeplot = pd.concat([moongmeplotpo,moongmeplotcom])\n",
    "print(len(moongmeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmoongmeplot = moongmeplot.set_index('created')\n",
    "tmoongmeplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'moon' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first post moon in GME time\n",
    "moongmepost = moongmepost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "moongmepost.rename(columns={'created':'first_gmepo_moon'},inplace=True)\n",
    "# get first comment moon in GME time\n",
    "moongmecom = moongmecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "moongmecom.rename(columns={'Publish Date':'first_gmecom_moon','Author':'author'},inplace=True)\n",
    "moongmepiece = pd.merge(moongmepost,moongmecom,left_on='author',right_on='author',how='outer')\n",
    "moongmepiece = pd.merge(moongmepiece[['author','first_gmepo_moon','first_gmecom_moon']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "moongmepiece = moongmepiece.merge(moongmecomfreq,left_on='author',right_on='Author',how='left')\n",
    "moongmepiece = moongmepiece.merge(moongmepofreq,left_on='author',right_on='author',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,j in moongmepiece['No_gmecom_moon'].iteritems():\n",
    "    if str(moongmepiece['No_gmecom_moon'][i]) == 'nan':\n",
    "        moongmepiece['No_gmecom_moon'][i] = 0\n",
    "for i,j in moongmepiece['No_gmepo_moon'].iteritems():\n",
    "    if str(moongmepiece['No_gmepo_moon'][i]) == 'nan':\n",
    "        moongmepiece['No_gmepo_moon'][i] = 0\n",
    "moongmepiece['No_gmemoon'] = moongmepiece['No_gmepo_moon'] + moongmepiece['No_gmecom_moon']\n",
    "moongmepiece['first_gmemoon']=''\n",
    "for i,r in moongmepiece.iterrows():\n",
    "    if str(moongmepiece['first_gmepo_moon'][i]) == 'NaT':\n",
    "        moongmepiece['first_gmemoon'][i] = moongmepiece['first_gmecom_moon'][i]\n",
    "    else:\n",
    "        if str(moongmepiece['first_gmecom_moon'][i]) == 'NaT':\n",
    "            moongmepiece['first_gmemoon'][i] = moongmepiece['first_gmepo_moon'][i]\n",
    "        else:\n",
    "            if moongmepiece['first_gmepo_moon'][i] < moongmepiece['first_gmecom_moon'][i]:\n",
    "                moongmepiece['first_gmemoon'][i] = moongmepiece['first_gmepo_moon'][i]\n",
    "            else:\n",
    "                moongmepiece['first_gmemoon'][i] = moongmepiece['first_gmecom_moon'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(moongmepiece))\n",
    "len(moongmepiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgm = moongmepiece.set_index('first_gmemoon')\n",
    "tfgm.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moongmepiece['firstgmemoon_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - moongmepiece['first_gmemoon']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='firstgmemoon_length ~ truecontent_length', data=moongmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_gmemoon ~ truecontent_length', data=moongmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstgmemoon_length', data=moongmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use moon before first gme\n",
    "len(moongmepiece[moongmepiece['first_gme'] > moongmepiece['first_gmemoon']])/5449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moongmepiece['gme_gmemoon_length'] = (moongmepiece['first_gmemoon'] - moongmepiece['first_gme']).apply(lambda x:x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moongmepiece['gme_gmemoon_length'].mean())\n",
    "print(moongmepiece['gme_gmemoon_length'].median())\n",
    "print(moongmepiece['gme_gmemoon_length'].std())\n",
    "print(moongmepiece['gme_gmemoon_length'].min())\n",
    "print(moongmepiece['gme_gmemoon_length'].max())\n",
    "np.percentile(moongmepiece['gme_gmemoon_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moongmepiece['gme_gmemoon_length'].value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgma = moongmepiece.set_index('first_in_wsb')\n",
    "tfgma.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gme_gmemoon_length ~ truecontent_length', data=moongmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used hold\n",
    "holdpost = peak1auposts[(peak1auposts.body.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False)) | (peak1auposts.title.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False))]\n",
    "holdcom = peak1aucomments[(~peak1aucomments.Body.isna()) & (peak1aucomments.Body.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False))]\n",
    "holdpost = holdpost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "holdcom = holdcom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del holdcom['author']\n",
    "holdcomfreq = pd.DataFrame(holdcom.groupby(['Author'])['Comment ID'].count())\n",
    "holdcomfreq.rename(columns={'Comment ID':'No_com_hold'},inplace=True)\n",
    "holdpofreq = pd.DataFrame(holdpost.groupby(['author'])['sub_id'].count())\n",
    "holdpofreq.rename(columns={'sub_id':'No_po_hold'},inplace=True)\n",
    "holdplotpo = holdpost[['created','sub_id']]\n",
    "holdplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "holdplotcom = holdcom[['Publish Date','Comment ID']]\n",
    "holdplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "holdplot = pd.concat([holdplotpo,holdplotcom])\n",
    "print(len(holdplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tholdplot = holdplot.set_index('created')\n",
    "tholdplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'hold' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first post hold time\n",
    "holdpost = holdpost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "holdpost.rename(columns={'created':'first_po_hold'},inplace=True)\n",
    "# get first comment hold time\n",
    "holdcom = holdcom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "holdcom.rename(columns={'Publish Date':'first_com_hold','Author':'author'},inplace=True)\n",
    "holdpiece = pd.merge(holdpost,holdcom,left_on='author',right_on='author',how='outer')\n",
    "holdpiece = pd.merge(holdpiece[['author','first_po_hold','first_com_hold']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "holdpiece = holdpiece.merge(holdcomfreq,left_on='author',right_on='Author',how='left')\n",
    "holdpiece = holdpiece.merge(holdpofreq,left_on='author',right_on='author',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,j in holdpiece['No_com_hold'].iteritems():\n",
    "    if str(holdpiece['No_com_hold'][i]) == 'nan':\n",
    "        holdpiece['No_com_hold'][i] = 0\n",
    "for i,j in holdpiece['No_po_hold'].iteritems():\n",
    "    if str(holdpiece['No_po_hold'][i]) == 'nan':\n",
    "        holdpiece['No_po_hold'][i] = 0\n",
    "holdpiece['No_hold'] = holdpiece['No_po_hold'] + holdpiece['No_com_hold']\n",
    "holdpiece['first_hold']=''\n",
    "for i,r in holdpiece.iterrows():\n",
    "    if str(holdpiece['first_po_hold'][i]) == 'NaT':\n",
    "        holdpiece['first_hold'][i] = holdpiece['first_com_hold'][i]\n",
    "    else:\n",
    "        if str(holdpiece['first_com_hold'][i]) == 'NaT':\n",
    "            holdpiece['first_hold'][i] = holdpiece['first_po_hold'][i]\n",
    "        else:\n",
    "            if holdpiece['first_po_hold'][i] < holdpiece['first_com_hold'][i]:\n",
    "                holdpiece['first_hold'][i] = holdpiece['first_po_hold'][i]\n",
    "            else:\n",
    "                holdpiece['first_hold'][i] = holdpiece['first_com_hold'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(holdpiece))\n",
    "len(holdpiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfh = holdpiece.set_index('first_hold')\n",
    "tfh.groupby(pd.Grouper(freq='M'))['author'].count().plot(label='number of authors by month')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdpiece['firsthold_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - holdpiece['first_hold']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firsthold_length ~ truecontent_length', data=holdpiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_hold ~ truecontent_length', data=holdpiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firsthold_length', data=holdpiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hold before first gme\n",
    "print(len(holdpiece[holdpiece['first_gme'] > holdpiece['first_hold']])/13055)\n",
    "# use moon at or after first gme\n",
    "print(len(holdpiece[holdpiece['first_gme'] <= holdpiece['first_hold']]))\n",
    "print(len(holdpiece[holdpiece['first_gme'] <= holdpiece['first_hold']])/13055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdaftergme = holdpiece[holdpiece['first_gme'] <= holdpiece['first_hold']]\n",
    "tholdaftergme = holdaftergme.set_index('first_in_wsb')\n",
    "tholdaftergme.groupby(pd.Grouper(freq='W'))['author'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5853/len(holdaftergme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firsthold_length ~ truecontent_length', data=holdaftergme).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used hold in GME content\n",
    "holdgmepost = peak1augmepo[(peak1augmepo.body.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False)) | (peak1augmepo.title.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False))]\n",
    "holdgmecom = peak1augmecom[(~peak1augmecom.Body_x.isna()) & (peak1augmecom.Body_x.str.contains('hold|HOLD|holding|Hold|Holding|HOLDING',case=False))]\n",
    "holdgmepost = holdgmepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "holdgmecom = holdgmecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del holdgmecom['author']\n",
    "holdgmecomfreq = pd.DataFrame(holdgmecom.groupby(['Author'])['Comment ID'].count())\n",
    "holdgmecomfreq.rename(columns={'Comment ID':'No_gmecom_hold'},inplace=True)\n",
    "holdgmepofreq = pd.DataFrame(holdgmepost.groupby(['author'])['sub_id'].count())\n",
    "holdgmepofreq.rename(columns={'sub_id':'No_gmepo_hold'},inplace=True)\n",
    "holdgmeplotpo = holdgmepost[['created','sub_id']]\n",
    "holdgmeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "holdgmeplotcom = holdgmecom[['Publish Date','Comment ID']]\n",
    "holdgmeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "holdgmeplot = pd.concat([holdgmeplotpo,holdgmeplotcom])\n",
    "print(len(holdgmeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tholdgmeplot = holdgmeplot.set_index('created')\n",
    "tholdgmeplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'hold' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first post hold in GME time\n",
    "holdgmepost = holdgmepost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "holdgmepost.rename(columns={'created':'first_gmepo_hold'},inplace=True)\n",
    "# get first comment hold in GME time\n",
    "holdgmecom = holdgmecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "holdgmecom.rename(columns={'Publish Date':'first_gmecom_hold','Author':'author'},inplace=True)\n",
    "holdgmepiece = pd.merge(holdgmepost,holdgmecom,left_on='author',right_on='author',how='outer')\n",
    "holdgmepiece = pd.merge(holdgmepiece[['author','first_gmepo_hold','first_gmecom_hold']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "holdgmepiece = holdgmepiece.merge(holdgmecomfreq,left_on='author',right_on='Author',how='left')\n",
    "holdgmepiece = holdgmepiece.merge(holdgmepofreq,left_on='author',right_on='author',how='left')\n",
    "\n",
    "for i,j in holdgmepiece['No_gmecom_hold'].iteritems():\n",
    "    if str(holdgmepiece['No_gmecom_hold'][i]) == 'nan':\n",
    "        holdgmepiece['No_gmecom_hold'][i] = 0\n",
    "for i,j in holdgmepiece['No_gmepo_hold'].iteritems():\n",
    "    if str(holdgmepiece['No_gmepo_hold'][i]) == 'nan':\n",
    "        holdgmepiece['No_gmepo_hold'][i] = 0\n",
    "holdgmepiece['No_gmehold'] = holdgmepiece['No_gmepo_hold'] + holdgmepiece['No_gmecom_hold']\n",
    "holdgmepiece['first_gmehold']=''\n",
    "for i,r in holdgmepiece.iterrows():\n",
    "    if str(holdgmepiece['first_gmepo_hold'][i]) == 'NaT':\n",
    "        holdgmepiece['first_gmehold'][i] = holdgmepiece['first_gmecom_hold'][i]\n",
    "    else:\n",
    "        if str(holdgmepiece['first_gmecom_hold'][i]) == 'NaT':\n",
    "            holdgmepiece['first_gmehold'][i] = holdgmepiece['first_gmepo_hold'][i]\n",
    "        else:\n",
    "            if holdgmepiece['first_gmepo_hold'][i] < holdgmepiece['first_gmecom_hold'][i]:\n",
    "                holdgmepiece['first_gmehold'][i] = holdgmepiece['first_gmepo_hold'][i]\n",
    "            else:\n",
    "                holdgmepiece['first_gmehold'][i] = holdgmepiece['first_gmecom_hold'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(holdgmepiece))\n",
    "len(holdgmepiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgh = holdgmepiece.set_index('first_gmehold')\n",
    "tfgh.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdgmepiece['firstgmehold_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - holdgmepiece['first_gmehold']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='firstgmehold_length ~ truecontent_length', data=holdgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_gmehold ~ truecontent_length', data=holdgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstgmehold_length', data=holdgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hold before first gme\n",
    "len(holdgmepiece[holdgmepiece['first_gme'] > holdgmepiece['first_gmehold']])/9969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdgmepiece['gme_gmehold_length'] = (holdgmepiece['first_gmehold'] - holdgmepiece['first_gme']).apply(lambda x:x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(holdgmepiece['gme_gmehold_length'].mean())\n",
    "print(holdgmepiece['gme_gmehold_length'].median())\n",
    "print(holdgmepiece['gme_gmehold_length'].std())\n",
    "print(holdgmepiece['gme_gmehold_length'].min())\n",
    "print(holdgmepiece['gme_gmehold_length'].max())\n",
    "np.percentile(holdgmepiece['gme_gmehold_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gme_gmehold_length ~ truecontent_length', data=holdgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used like the stock\n",
    "likepost = peak1auposts[(peak1auposts.body.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False)) | (peak1auposts.title.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False))]\n",
    "likecom = peak1aucomments[(~peak1aucomments.Body.isna()) & (peak1aucomments.Body.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False))]\n",
    "likepost = likepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "likecom = likecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del likecom['author']\n",
    "likecomfreq = pd.DataFrame(likecom.groupby(['Author'])['Comment ID'].count())\n",
    "likecomfreq.rename(columns={'Comment ID':'No_com_like'},inplace=True)\n",
    "likepofreq = pd.DataFrame(likepost.groupby(['author'])['sub_id'].count())\n",
    "likepofreq.rename(columns={'sub_id':'No_po_like'},inplace=True)\n",
    "likeplotpo = likepost[['created','sub_id']]\n",
    "likeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "likeplotcom = likecom[['Publish Date','Comment ID']]\n",
    "likeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "likeplot = pd.concat([likeplotpo,likeplotcom])\n",
    "print(len(likeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlikeplot = likeplot.set_index('created')\n",
    "tlikeplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'like the stock' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first post like the stock time\n",
    "likepost = likepost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "likepost.rename(columns={'created':'first_po_like'},inplace=True)\n",
    "# get first comment like the stock time\n",
    "likecom = likecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "likecom.rename(columns={'Publish Date':'first_com_like','Author':'author'},inplace=True)\n",
    "likepiece = pd.merge(likepost,likecom,left_on='author',right_on='author',how='outer')\n",
    "likepiece = pd.merge(likepiece[['author','first_po_like','first_com_like']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "likepiece = likepiece.merge(likecomfreq,left_on='author',right_on='Author',how='left')\n",
    "likepiece = likepiece.merge(likepofreq,left_on='author',right_on='author',how='left')\n",
    "\n",
    "for i,j in likepiece['No_com_like'].iteritems():\n",
    "    if str(likepiece['No_com_like'][i]) == 'nan':\n",
    "        likepiece['No_com_like'][i] = 0\n",
    "for i,j in likepiece['No_po_like'].iteritems():\n",
    "    if str(likepiece['No_po_like'][i]) == 'nan':\n",
    "        likepiece['No_po_like'][i] = 0\n",
    "likepiece['No_like'] = likepiece['No_po_like'] + likepiece['No_com_like']\n",
    "likepiece['first_like']=''\n",
    "for i,r in likepiece.iterrows():\n",
    "    if str(likepiece['first_po_like'][i]) == 'NaT':\n",
    "        likepiece['first_like'][i] = likepiece['first_com_like'][i]\n",
    "    else:\n",
    "        if str(likepiece['first_com_like'][i]) == 'NaT':\n",
    "            likepiece['first_like'][i] = likepiece['first_po_like'][i]\n",
    "        else:\n",
    "            if likepiece['first_po_like'][i] < likepiece['first_com_like'][i]:\n",
    "                likepiece['first_like'][i] = likepiece['first_po_like'][i]\n",
    "            else:\n",
    "                likepiece['first_like'][i] = likepiece['first_com_like'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(likepiece))\n",
    "len(likepiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfl = likepiece.set_index('first_like')\n",
    "tfl.groupby(pd.Grouper(freq='M'))['author'].count().plot(label='number of authors by month')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likepiece['firstlike_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - likepiece['first_like']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstlike_length ~ truecontent_length', data=likepiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_like ~ truecontent_length', data=likepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstlike_length', data=likepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use like the stock before first gme\n",
    "print(len(likepiece[likepiece['first_gme'] > likepiece['first_like']])/2869)\n",
    "# use like the stock at or after first gme\n",
    "print(len(likepiece[likepiece['first_gme'] <= likepiece['first_like']]))\n",
    "print(len(likepiece[likepiece['first_gme'] <= likepiece['first_like']])/2869)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeaftergme = likepiece[likepiece['first_gme'] <= likepiece['first_like']]\n",
    "tlikeaftergme = likeaftergme.set_index('first_in_wsb')\n",
    "tlikeaftergme.groupby(pd.Grouper(freq='W'))['author'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1078/len(likeaftergme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstlike_length ~ truecontent_length', data=likeaftergme).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used like the stock in GME content\n",
    "likegmepost = peak1augmepo[(peak1augmepo.body.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False)) | (peak1augmepo.title.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False))]\n",
    "likegmecom = peak1augmecom[(~peak1augmecom.Body_x.isna()) & (peak1augmecom.Body_x.str.contains('like the stock|Like The Stock|LIKE THE STOCK',case=False))]\n",
    "likegmepost = likegmepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "likegmecom = likegmecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del likegmecom['author']\n",
    "likegmecomfreq = pd.DataFrame(likegmecom.groupby(['Author'])['Comment ID'].count())\n",
    "likegmecomfreq.rename(columns={'Comment ID':'No_gmecom_like'},inplace=True)\n",
    "likegmepofreq = pd.DataFrame(likegmepost.groupby(['author'])['sub_id'].count())\n",
    "likegmepofreq.rename(columns={'sub_id':'No_gmepo_like'},inplace=True)\n",
    "likegmeplotpo = likegmepost[['created','sub_id']]\n",
    "likegmeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "likegmeplotcom = likegmecom[['Publish Date','Comment ID']]\n",
    "likegmeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "likegmeplot = pd.concat([likegmeplotpo,likegmeplotcom])\n",
    "print(len(likegmeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlikegmeplot = likegmeplot.set_index('created')\n",
    "tlikegmeplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'like the stock' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first post like the stock in GME time\n",
    "likegmepost = likegmepost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "likegmepost.rename(columns={'created':'first_gmepo_like'},inplace=True)\n",
    "# get first comment like the stock in GME time\n",
    "likegmecom = likegmecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "likegmecom.rename(columns={'Publish Date':'first_gmecom_like','Author':'author'},inplace=True)\n",
    "likegmepiece = pd.merge(likegmepost,likegmecom,left_on='author',right_on='author',how='outer')\n",
    "likegmepiece = pd.merge(likegmepiece[['author','first_gmepo_like','first_gmecom_like']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "likegmepiece = likegmepiece.merge(likegmecomfreq,left_on='author',right_on='Author',how='left')\n",
    "likegmepiece = likegmepiece.merge(likegmepofreq,left_on='author',right_on='author',how='left')\n",
    "\n",
    "for i,j in likegmepiece['No_gmecom_like'].iteritems():\n",
    "    if str(likegmepiece['No_gmecom_like'][i]) == 'nan':\n",
    "        likegmepiece['No_gmecom_like'][i] = 0\n",
    "for i,j in likegmepiece['No_gmepo_like'].iteritems():\n",
    "    if str(likegmepiece['No_gmepo_like'][i]) == 'nan':\n",
    "        likegmepiece['No_gmepo_like'][i] = 0\n",
    "likegmepiece['No_gmelike'] = likegmepiece['No_gmepo_like'] + likegmepiece['No_gmecom_like']\n",
    "likegmepiece['first_gmelike']=''\n",
    "for i,r in likegmepiece.iterrows():\n",
    "    if str(likegmepiece['first_gmepo_like'][i]) == 'NaT':\n",
    "        likegmepiece['first_gmelike'][i] = likegmepiece['first_gmecom_like'][i]\n",
    "    else:\n",
    "        if str(likegmepiece['first_gmecom_like'][i]) == 'NaT':\n",
    "            likegmepiece['first_gmelike'][i] = likegmepiece['first_gmepo_like'][i]\n",
    "        else:\n",
    "            if likegmepiece['first_gmepo_like'][i] < likegmepiece['first_gmecom_like'][i]:\n",
    "                likegmepiece['first_gmelike'][i] = likegmepiece['first_gmepo_like'][i]\n",
    "            else:\n",
    "                likegmepiece['first_gmelike'][i] = likegmepiece['first_gmecom_like'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(likegmepiece))\n",
    "len(likegmepiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgl = likegmepiece.set_index('first_gmelike')\n",
    "tfgl.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likegmepiece['firstgmelike_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - likegmepiece['first_gmelike']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='firstgmelike_length ~ truecontent_length', data=likegmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_gmelike ~ truecontent_length', data=likegmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstgmelike_length', data=likegmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use like the stock before first gme\n",
    "len(likegmepiece[likegmepiece['first_gme'] > likegmepiece['first_gmelike']])/1981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likegmepiece['gme_gmelike_length'] = (likegmepiece['first_gmelike'] - likegmepiece['first_gme']).apply(lambda x:x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likegmepiece['gme_gmelike_length'].mean())\n",
    "print(likegmepiece['gme_gmelike_length'].median())\n",
    "print(likegmepiece['gme_gmelike_length'].std())\n",
    "print(likegmepiece['gme_gmelike_length'].min())\n",
    "print(likegmepiece['gme_gmelike_length'].max())\n",
    "np.percentile(likegmepiece['gme_gmelike_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gme_gmelike_length ~ truecontent_length', data=likegmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used retard\n",
    "retardpost = peak1auposts[(peak1auposts.body.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False)) | (peak1auposts.title.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False))]\n",
    "retardcom = peak1aucomments[(~peak1aucomments.Body.isna()) & (peak1aucomments.Body.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False))]\n",
    "retardpost = retardpost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "retardcom = retardcom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del retardcom['author']\n",
    "retardcomfreq = pd.DataFrame(retardcom.groupby(['Author'])['Comment ID'].count())\n",
    "retardcomfreq.rename(columns={'Comment ID':'No_com_retard'},inplace=True)\n",
    "retardpofreq = pd.DataFrame(retardpost.groupby(['author'])['sub_id'].count())\n",
    "retardpofreq.rename(columns={'sub_id':'No_po_retard'},inplace=True)\n",
    "retardplotpo = retardpost[['created','sub_id']]\n",
    "retardplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "retardplotcom = retardcom[['Publish Date','Comment ID']]\n",
    "retardplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "retardplot = pd.concat([retardplotpo,retardplotcom])\n",
    "print(len(retardplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tretardplot = retardplot.set_index('created')\n",
    "tretardplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'retard the stock' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first post retard the stock time\n",
    "retardpost = retardpost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "retardpost.rename(columns={'created':'first_po_retard'},inplace=True)\n",
    "# get first comment retard the stock time\n",
    "retardcom = retardcom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "retardcom.rename(columns={'Publish Date':'first_com_retard','Author':'author'},inplace=True)\n",
    "retardpiece = pd.merge(retardpost,retardcom,left_on='author',right_on='author',how='outer')\n",
    "retardpiece = pd.merge(retardpiece[['author','first_po_retard','first_com_retard']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "retardpiece = retardpiece.merge(retardcomfreq,left_on='author',right_on='Author',how='left')\n",
    "retardpiece = retardpiece.merge(retardpofreq,left_on='author',right_on='author',how='left')\n",
    "\n",
    "for i,j in retardpiece['No_com_retard'].iteritems():\n",
    "    if str(retardpiece['No_com_retard'][i]) == 'nan':\n",
    "        retardpiece['No_com_retard'][i] = 0\n",
    "for i,j in retardpiece['No_po_retard'].iteritems():\n",
    "    if str(retardpiece['No_po_retard'][i]) == 'nan':\n",
    "        retardpiece['No_po_retard'][i] = 0\n",
    "retardpiece['No_retard'] = retardpiece['No_po_retard'] + retardpiece['No_com_retard']\n",
    "retardpiece['first_retard']=''\n",
    "for i,r in retardpiece.iterrows():\n",
    "    if str(retardpiece['first_po_retard'][i]) == 'NaT':\n",
    "        retardpiece['first_retard'][i] = retardpiece['first_com_retard'][i]\n",
    "    else:\n",
    "        if str(retardpiece['first_com_retard'][i]) == 'NaT':\n",
    "            retardpiece['first_retard'][i] = retardpiece['first_po_retard'][i]\n",
    "        else:\n",
    "            if retardpiece['first_po_retard'][i] < retardpiece['first_com_retard'][i]:\n",
    "                retardpiece['first_retard'][i] = retardpiece['first_po_retard'][i]\n",
    "            else:\n",
    "                retardpiece['first_retard'][i] = retardpiece['first_com_retard'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retardpiece))\n",
    "len(retardpiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr = retardpiece.set_index('first_retard')\n",
    "tfr.groupby(pd.Grouper(freq='M'))['author'].count().plot(label='number of authors by month')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retardpiece['firstretard_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - retardpiece['first_retard']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstretard_length ~ truecontent_length', data=retardpiece).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_retard ~ truecontent_length', data=retardpiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstretard_length', data=retardpiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retard before first gme\n",
    "print(len(retardpiece[retardpiece['first_gme'] > retardpiece['first_retard']])/7975)\n",
    "# use retard at or after first gme\n",
    "print(len(retardpiece[retardpiece['first_gme'] <= retardpiece['first_retard']]))\n",
    "print(len(retardpiece[retardpiece['first_gme'] <= retardpiece['first_retard']])/7975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retardaftergme = retardpiece[retardpiece['first_gme'] <= retardpiece['first_retard']]\n",
    "tretardaftergme = retardaftergme.set_index('first_in_wsb')\n",
    "tretardaftergme.groupby(pd.Grouper(freq='W'))['author'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2663/len(retardaftergme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result10 = sm.ols(formula='firstretard_length ~ truecontent_length', data=retardaftergme).fit()\n",
    "print(result10.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people used retard in GME content\n",
    "retardgmepost = peak1augmepo[(peak1augmepo.body.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False)) | (peak1augmepo.title.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False))]\n",
    "retardgmecom = peak1augmecom[(~peak1augmecom.Body_x.isna()) & (peak1augmecom.Body_x.str.contains('retard|retards|retarded|Retard|RETARD|RETARDS|Retards|RETARDED',case=False))]\n",
    "retardgmepost = retardgmepost[['author','created','sub_id']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "retardgmecom = retardgmecom[['Author','Publish Date','Comment ID']].merge(sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='Author',right_on='author',how='inner')\n",
    "del retardgmecom['author']\n",
    "retardgmecomfreq = pd.DataFrame(retardgmecom.groupby(['Author'])['Comment ID'].count())\n",
    "retardgmecomfreq.rename(columns={'Comment ID':'No_gmecom_retard'},inplace=True)\n",
    "retardgmepofreq = pd.DataFrame(retardgmepost.groupby(['author'])['sub_id'].count())\n",
    "retardgmepofreq.rename(columns={'sub_id':'No_gmepo_retard'},inplace=True)\n",
    "retardgmeplotpo = retardgmepost[['created','sub_id']]\n",
    "retardgmeplotpo.rename(columns={'sub_id':'id'},inplace=True)\n",
    "retardgmeplotcom = retardgmecom[['Publish Date','Comment ID']]\n",
    "retardgmeplotcom.rename(columns={'Publish Date':'created','Comment ID':'id'},inplace=True)\n",
    "retardgmeplot = pd.concat([retardgmeplotpo,retardgmeplotcom])\n",
    "print(len(retardgmeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tretardgmeplot = retardgmeplot.set_index('created')\n",
    "tretardgmeplot.groupby(pd.Grouper(freq='M'))['id'].count().plot(label=\"number of pieces used 'retard' by month\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get first post retard in GME time\n",
    "retardgmepost = retardgmepost.sort_values(\"created\").drop_duplicates(\"author\",keep='first')\n",
    "retardgmepost.rename(columns={'created':'first_gmepo_retard'},inplace=True)\n",
    "# get first comment retard in GME time\n",
    "retardgmecom = retardgmecom.sort_values(\"Publish Date\").drop_duplicates(\"Author\",keep='first')\n",
    "retardgmecom.rename(columns={'Publish Date':'first_gmecom_retard','Author':'author'},inplace=True)\n",
    "retardgmepiece = pd.merge(retardgmepost,retardgmecom,left_on='author',right_on='author',how='outer')\n",
    "retardgmepiece = pd.merge(retardgmepiece[['author','first_gmepo_retard','first_gmecom_retard']],sumpeak1au[['author','first_in_wsb','first_gme','last_gme','gmepost_score','gmecom_score','truecontent_length','gme_length','firstgme_length','No_gme']]\n",
    "                         ,left_on='author',right_on='author',how='inner')\n",
    "retardgmepiece = retardgmepiece.merge(retardgmecomfreq,left_on='author',right_on='Author',how='left')\n",
    "retardgmepiece = retardgmepiece.merge(retardgmepofreq,left_on='author',right_on='author',how='left')\n",
    "\n",
    "for i,j in retardgmepiece['No_gmecom_retard'].iteritems():\n",
    "    if str(retardgmepiece['No_gmecom_retard'][i]) == 'nan':\n",
    "        retardgmepiece['No_gmecom_retard'][i] = 0\n",
    "for i,j in retardgmepiece['No_gmepo_retard'].iteritems():\n",
    "    if str(retardgmepiece['No_gmepo_retard'][i]) == 'nan':\n",
    "        retardgmepiece['No_gmepo_retard'][i] = 0\n",
    "retardgmepiece['No_gmeretard'] = retardgmepiece['No_gmepo_retard'] + retardgmepiece['No_gmecom_retard']\n",
    "retardgmepiece['first_gmeretard']=''\n",
    "for i,r in retardgmepiece.iterrows():\n",
    "    if str(retardgmepiece['first_gmepo_retard'][i]) == 'NaT':\n",
    "        retardgmepiece['first_gmeretard'][i] = retardgmepiece['first_gmecom_retard'][i]\n",
    "    else:\n",
    "        if str(retardgmepiece['first_gmecom_retard'][i]) == 'NaT':\n",
    "            retardgmepiece['first_gmeretard'][i] = retardgmepiece['first_gmepo_retard'][i]\n",
    "        else:\n",
    "            if retardgmepiece['first_gmepo_retard'][i] < retardgmepiece['first_gmecom_retard'][i]:\n",
    "                retardgmepiece['first_gmeretard'][i] = retardgmepiece['first_gmepo_retard'][i]\n",
    "            else:\n",
    "                retardgmepiece['first_gmeretard'][i] = retardgmepiece['first_gmecom_retard'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retardgmepiece))\n",
    "len(retardgmepiece)/30013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfgr = retardgmepiece.set_index('first_gmeretard')\n",
    "tfgr.groupby(pd.Grouper(freq='W'))['author'].count().plot(label='number of authors by week')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retardgmepiece['firstgmeretard_length'] = (datetime(year=2021, month=6, day=1, hour=0, minute=0, second=0) - retardgmepiece['first_gmeretard']).apply(lambda x:x.days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='firstgmeretard_length ~ truecontent_length', data=retardgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='No_gmeretard ~ truecontent_length', data=retardgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gmepost_score ~ firstgmeretard_length', data=retardgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retard before first gme\n",
    "len(retardgmepiece[retardgmepiece['first_gme'] > retardgmepiece['first_gmeretard']])/5383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retardgmepiece['gme_gmeretard_length'] = (retardgmepiece['first_gmeretard'] - retardgmepiece['first_gme']).apply(lambda x:x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retardgmepiece['gme_gmeretard_length'].mean())\n",
    "print(retardgmepiece['gme_gmeretard_length'].median())\n",
    "print(retardgmepiece['gme_gmeretard_length'].std())\n",
    "print(retardgmepiece['gme_gmeretard_length'].min())\n",
    "print(retardgmepiece['gme_gmeretard_length'].max())\n",
    "np.percentile(retardgmepiece['gme_gmeretard_length'], 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result11 = sm.ols(formula='gme_gmeretard_length ~ truecontent_length', data=retardgmepiece).fit()\n",
    "print(result11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumpeak1au.loc[sumpeak1au[\"Author\"].isna(),\"Author\"] = sumpeak1au[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sumpeak1au.to_csv('/Users/elaine/Desktop/poscore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1aucomgme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1ausamcom['Publish Date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1ausamcom['Publish Date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au[\"first_posted_in_wsb\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comscoredf = pd.DataFrame(peak1ausamcom.groupby(['Author'])['Score'].mean())\n",
    "sumpeak1au = sumpeak1au.merge(comscoredf,left_on='author',right_on='Author',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumpeak1au[\"score\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = sm.ols(formula='score ~ posted_length', data=sumpeak1au).fit()\n",
    "print(result1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
